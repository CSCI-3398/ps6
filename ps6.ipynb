{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 6: Deep learning ASR\n",
    "\n",
    "In this problem set, you'll be building a deep neural network for recognizing a set of 10 commands of the sort that could be used to control a robot or character in a videogame. You won't be writing a lot of code for this problem set, but you will answer some questions and experiment a bit with deep learning architectures.\n",
    "\n",
    "The deliverable for this assignment, due on **Wednesday, November 11, 11:59pm Boston time**, is this Jupyter notebook. For this assignment, we will need to see the output of all your code, but we won't have time to run your code since it could take several hours. I recommend that you do `Kernel-> Restart and run all` a few hours before the assignment is due so that all the code can run and complete before the deadline.**\n",
    "\n",
    "**Important Notes**\n",
    "\n",
    "* Unless I say otherwise, you don't need to change any of the code blocks before you run them. A lot of what you will do for this problem set is answer questions and run the existing code.\n",
    "\n",
    "* Even though there is deep learning going on here, you should be able to complete this on your own computer. If you can't (e.g., it doesn't work on Windows, if you can't get Python 3.8 installed), then use [Google Colab](https://colab.research.google.com). There's a version of this notebook in the repo (`ps6-colab.ipynb`) that includes code for uploading and storing files so that they can be used with Colab.\n",
    "\n",
    "* The code can take a long time to run! If you start this the day it's due, it's likely that it won't be done by 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing libraries and checking compatibility\n",
    "\n",
    "You can install librosa with Python 3.7, but I have not been able to make librosa work on anything under Python 3.8, so you should use Python 3.8 or higher for this problem set. Not aure which Python you are running? Go up to `Help -> About` to find out. If you are running < 3.8 and you are not sure what to do, download the latest Anaconda, install librosa, and then launch Jupyter from Anaconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Obtaining and Examining the data\n",
    "\n",
    "We are going to build an ASR model that will recognize 10 speech commands. The data we'll be using is a subset of the [Google Speech Commands dataset](https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html). \n",
    "\n",
    "1. [Download the data from here](http://cs.bc.edu/~prudhome/speech_commands.tgz)\n",
    "\n",
    "2. Move the file you downloaded to this exact directory that you are in right now.\n",
    "\n",
    "3. Untar and unzip the file (e.g., with ``tar xzf speech_commands.tgz`` or probably by double clicking it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code block below, I'm just visualizing one of the waves for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_path = 'speech_commands/'\n",
    "\n",
    "# Use Librosa to read in the wav file.\n",
    "samples, sample_rate = librosa.load(train_audio_path+ 'yes/0a7c2a8d_nohash_0.wav', sr = 16000)\n",
    "\n",
    "# Plot the wav file.\n",
    "fig = plt.figure(figsize=(14, 8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.set_title('Raw wave of ' + train_audio_path + '/yes/0a7c2a8d_nohash_0.wav')\n",
    "ax1.set_xlabel('time')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax1.plot(np.linspace(0, sample_rate/len(samples), sample_rate), samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! It looks like a normal wave file. Let's look at a spectrogram. The code below will make a mel spectrogram, which is a different visualization than we're used to seeing in Praat, but you should be able to see a clear vowel (the \"e\" in \"yes\") followed by a clear fricative (the \"s\" in \"yes\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectro = librosa.feature.melspectrogram(y=samples, sr=sample_rate, n_mels=128, fmax=8000)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(librosa.power_to_db(spectro, ref=np.max), y_axis='mel', fmax=10000, x_axis='time')\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "plt.title('Mel spectrogram')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Using the code above, write code in the code block below to open a  `.wav` file for different command, then produce its mel spectrogram. Does the spectrogram look more or less as you would expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write your code for Q1 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how many training examples there are for each command! I filtered out some commands in the original data set that were too short or that were sampled at something other than 16kHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels=[\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\"]\n",
    "\n",
    "#find count of each label and plot bar graph\n",
    "no_of_recordings=[]\n",
    "for label in labels:\n",
    "    waves = [f for f in os.listdir(train_audio_path + label) if f.endswith('.wav')]\n",
    "    no_of_recordings.append(len(waves))\n",
    "    \n",
    "#plot\n",
    "plt.figure(figsize=(30,5))\n",
    "index = np.arange(len(labels))\n",
    "plt.bar(index, no_of_recordings)\n",
    "plt.xlabel('Commands', fontsize=12)\n",
    "plt.ylabel('No of recordings', fontsize=12)\n",
    "plt.xticks(index, labels, fontsize=15, rotation=60)\n",
    "plt.title('No. of recordings for each command')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Which command has the most examples? Which has the least? (Note that you might not be able to tell which has the least just by looking at the graph. You might have to print out some of the values from the code block above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your answer to Q2 here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data preparation\n",
    "\n",
    "The code block below reads in each `.wav` file, downsamples it to 8000Hz, and then saves the data to the `all_wavs` list and its label (i.e., the name of the command) to the `all_label` list. \n",
    "\n",
    "**This code takes some time to run! Run the block below, and then check back every 5 minutes or so.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-- Wait to execute the next code block until after you see a number in between these square brackets.\n",
    "# This will take a little while.\n",
    "all_wave = []\n",
    "all_label = []\n",
    "for label in labels:\n",
    "    print(label)\n",
    "    waves = [f for f in os.listdir(train_audio_path + '/'+ label) if f.endswith('.wav')]\n",
    "    for wav in waves:\n",
    "        samples, sample_rate = librosa.load(train_audio_path + '/' + label + '/' + wav, sr = 16000)\n",
    "        samples = librosa.resample(samples, sample_rate, 8000)\n",
    "        all_wave.append(samples)\n",
    "        all_label.append(label)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, computers don't like dealing with strings. For this reason, we are going to associate each label (\"yes\", \"no\", \"up\", \"down\") with a unique integer. The code below does this for you!\n",
    "\n",
    "### **Q3: Add a few lines of code at the bottom of the following code block to help you figure out the integer value associated with each label.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User the sklearn preprocessing function that will do this for you.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# This makes a list y that contains, for each example in the all_label list,\n",
    "# the integer associated with that label\n",
    "y=le.fit_transform(all_label)  # just like all_label but with integers rather than strings\n",
    "\n",
    "# This just gives you the labels (i.e., the classes).\n",
    "classes= list(le.classes_)\n",
    "\n",
    "### Add code here to print out each label and its integer representation. ###\n",
    "# PRINT OUT EACH LABEL ONLY ONCE! I expect to see something like this:\n",
    "# 9 yes\n",
    "# 5 on\n",
    "# 0 down\n",
    "# etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that neural nets like to deal with vectors. For that reason, we are doing to convert each label from an integer to a **one-hot vector**. In other words, the output for the neural net will be a vector whose length is the number of labels (10) with zeros in all places except for the one associated with the predicted label. For instance, if the integer associated with \"down\" is 0, then the one-hot vector would be `[1 0 0 0 0 0 0 0 0 0]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the original integer label and the string label for one training example\n",
    "print(y[0], all_label[0])\n",
    "\n",
    "# convert integer labels to one-hot vectors\n",
    "from keras.utils import np_utils\n",
    "y=np_utils.to_categorical(y, num_classes=len(labels))\n",
    "\n",
    "# print an example one-hot vector and the string label for that training example\n",
    "print(y[0], all_label[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we have our audio data in a 2D \"list of lists\". We need to make it into the shape the neural net will be expecting, which is 3D matrix (21K example commands with 8000 samples per command with each sample as its own little vector). The code below will do this for you, and you can see how the shape changes from the original 2D list of lists to the 3D matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_wave[0])\n",
    "all_wave = np.array(all_wave).reshape(-1,8000,1)\n",
    "print(all_wave[0])\n",
    "np.shape(all_wave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will split the data into a training and validation set using the sklearn `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(np.array(all_wave),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initializing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using Keras, which is a Python API for TensorFlow, a widely-used deep learning library. Below you will see a lot of different import statements for the various components of our deep learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are making a convolutional neural net (CNN). There are many layers in our architecture, as you can see in the code block below: 3 convolutional layers (`Conv1D`), a \"flattening\" layer (`Flatten`), one dense (`Dense`) layer, and the dense output layer. What does all this mean? \n",
    "\n",
    "\n",
    "### Convolutional layers\n",
    "We'll be talking about CNNs a little bit in class on Thursday. It's often easier to conceptualize CNNs as they are applied to images rather than speech. You'll remember from class that animation where a small \"filter\" matrix of weights (also called a \"kernel\") sweeps across the input matrix, and the output from each step in the sweep is run through an activation function and that output is pooled. The gif below gives you an idea of what this is like (minus the activation function), where the weights in the filter are are applied to elements in the matrix underneath, and the sum is entered into the output element (\"convolved feature\").\n",
    "\n",
    "![convolution](convolution.gif)\n",
    "\n",
    "\n",
    "The arguments to the `Conv1D` layers give us the parameters of this process. The first argument is the number of filters/kernels. The second argument is the size or the dimensions of that filter. The `strides` argument is how far forward the filter jumps as it sweeps.\n",
    "\n",
    "In the above gif, the pooling was a sum. In the code below, we use max pooling, which means that you just take the maximum value of the output of the filter.\n",
    "\n",
    "Finally, `Dropout` tells the model to randomly set some percentage of the current matrix of features to 0. Dropout is a kind of regularization, or a method for avoiding overfitting. By randomly throwing out some features, dropout forces the model to look for relationships in all the features and to avoid becoming too dependent on a small of number of specific features.\n",
    "\n",
    "### Other layers\n",
    "The `Flatten` layer is straightfoward: it just turns multidimensional input into a 1D vector.\n",
    "\n",
    "The `Dense` layers are the kind of layers you think of when you think of a basic neural network. The input is a vector, and every input element goes through every node (\"fully connected\"). \n",
    "\n",
    "The final (output) layer is a dense layer where the length of the output is equal to the number of classes. The `softmax` activation function converts the output into (more or less) a probability distribution over the classes, and the predicted class is the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "\n",
    "inputs = Input(shape=(8000,1))\n",
    "\n",
    "#First Conv1D layer\n",
    "conv = Conv1D(8,13, padding='valid', activation='relu', strides=1)(inputs)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Second Conv1D layer\n",
    "conv = Conv1D(16, 11, padding='valid', activation='relu', strides=1)(conv)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Third Conv1D layer\n",
    "conv = Conv1D(32, 9, padding='valid', activation='relu', strides=1)(conv)\n",
    "conv = MaxPooling1D(3)(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "#Flatten layer\n",
    "conv = Flatten()(conv)\n",
    "\n",
    "#Dense Layer 1\n",
    "conv = Dense(128, activation='relu')(conv)\n",
    "conv = Dropout(0.3)(conv)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(len(labels), activation='softmax')(conv)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other model parameters and components\n",
    "\n",
    "The `compile()` method is how we set the loss function we want to use (e.g., cross entropy, mean squared error), the optimized (adam, sgd), and what metrics we want to report (accuracy).\n",
    "\n",
    "`EarlyStopping` tells the model it can stop training if, in this case, the loss on the validation set doesn't improve by more than 0.0001 for 10 epoches.\n",
    "\n",
    "`ModelCheckpoint` allows us to save out the best performing model so that we can reload it later and test new sata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.0001) \n",
    "mc = ModelCheckpoint('best_model.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Training the model\n",
    "\n",
    "The line of code below trains the model for 10 epochs. This means that it will go through the whole dataset 10 times. The batch size is 32, meaning that it will update the weights after every 32 input examples. The `callbacks` are just the arguments for early stopping and saving out a model checkpoint , described above. \n",
    "\n",
    "After you run the code below, make sure it's going, and then go have a coffee or take a walk. It will be a while -- maybe 20 or 30 minutes depending on your machine. You'll know its done when you see the `MODEL TRAINING COMPLETE` message printed out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model.fit(x_tr, y_tr ,epochs=10, callbacks=[es,mc], batch_size=32, validation_data=(x_val,y_val))\n",
    "\n",
    "print(\"MODEL TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Reviewing the model\n",
    "\n",
    "Okay, you have trained a CNN to recognize ten spoken commands! In the output above, you'll see that the accuracy started off low but steadily increased, while the loss started out high, then steadily decreased.\n",
    "\n",
    "Let's plot the loss on the training and on the validation set to see whether the model is learning over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(history.history['loss'], label='train') \n",
    "plt.plot(history.history['val_loss'], label='test') \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: In the code block below, write code that will print out a graph like the one above but for accuracy on the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMILY REMOVE THIS CODE\n",
    "plt.plot(history.history['accuracy'], label='train') \n",
    "plt.plot(history.history['val_accuracy'], label='test') \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Testing the model with your own voice\n",
    "\n",
    "Using Praat, record yourself saying each of the 10 commands. Save each one out as a `WAV` file into the same directory as this Jupyter notebook, naming each one with its label, e.g., `yes.wav` or `start.wav`. \n",
    "\n",
    "Then you can run the code blocks below to test the model on your own recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows us to load the model we just trained, above.\n",
    "\n",
    "from keras.models import load_model\n",
    "model=load_model('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a little function that takes a recording\n",
    "# and predicts which command it is.\n",
    "\n",
    "def predict(audio):\n",
    "    prob=model.predict(audio.reshape(1,8000,1))\n",
    "    index=np.argmax(prob[0])\n",
    "    return classes[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will read in one of your recorded commands \n",
    "# at the necessary 8000Hz sampling rate.\n",
    "\n",
    "# EMILY FIX THIS\n",
    "#for l in labels:\n",
    "for l in [\"go\", \"left\", \"stop\", \"up\"]:\n",
    "    wavefile = l + \".wav\"\n",
    "    samples, sample_rate = librosa.load(wavefile, sr = 44100)\n",
    "    samples = librosa.resample(samples, sample_rate, 8000)\n",
    "\n",
    "    # This is a dumb way to trim or pad your recording to 1 second,\n",
    "    # which is necessary for the way I've set this up.\n",
    "    # Basically: just chop off the difference on either end, or\n",
    "    # if it's too short, add a bunch of zeros at the end.\n",
    "\n",
    "    if len(samples) > 8000:\n",
    "        toremove = int( (len(samples) - 8000)/2)\n",
    "        if len(samples) % 2 == 0:\n",
    "            samples = samples[toremove:-toremove]\n",
    "        else:\n",
    "            samples = samples[toremove:-toremove-1]\n",
    "    else:\n",
    "        samples = np.pad(samples, (0, 8000-len(samples)))\n",
    "    print(\"actual:\", l, \"\\tpredicted:\", predict(samples))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Report the conditions under which you made the recording: location, kind of microphone, whether you were wearing a mask. What accuracy did the recognizer yield on your recorded commands? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your answer to Q5 here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Refining your model\n",
    "\n",
    "I made many more or less arbitrary choices about how to train the model, above. \n",
    "\n",
    "* I decided to do only 10 epochs of training. The model seemed to be still improving when I stopped, so what would happen if I trained for more epochs? \n",
    "* There are three convolution layers and 2 dense layers -- why not more or fewer? \n",
    "* Dropout was 0.3, but what if it had been more or less? \n",
    "* What if we used a different activation function or a different measure of loss or a different pooling method?\n",
    "* What if we change the various parameters for the convolutional layers?\n",
    "\n",
    "### Q6: In the code block(s) below, I'd like you to build, fit, and test a new model by changing one or more of these parameters, above. Mostly you will be copying and pasting the code above starting with Step 4, and then making slight modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write your code here!\n",
    "### Feel free to include the plots, as well.\n",
    "### Don't forget to test on your voice data, too,\n",
    "### and feel free to create more challenging recordings \n",
    "### (e.g., by wearing a mask, recording in a noisy environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7: How did your new model perform on the validation set and on your own voice test set compared to the model we originally created above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Enter your answer to Q7 here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Submission\n",
    "The deliverable for this assignment, due on **Wednesday, November 11, 11:59pm Boston time**, is this Jupyter notebook. \n",
    "\n",
    "**IMPORTANT:** For this assignment, we will need to see the output of all your code, but we will not have time to actually run your code since it could take several hours. I recommend that you do `Kernel-> Restart and run all` a few hours before the assignment is due so that all the code can run and complete before the deadline.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
